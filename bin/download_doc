#!/usr/bin/env python3

import os
import re
import sys
from argparse import ArgumentParser
from logging import DEBUG, INFO, StreamHandler, getLogger
from pathlib import Path
from posixpath import basename
from textwrap import dedent
from typing import List, Tuple
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from slugify import slugify

logger = getLogger(__name__)


def configure_logging(debug: bool) -> None:
    level = DEBUG if debug else INFO
    logger.setLevel(level)
    ch = StreamHandler()
    logger.addHandler(ch)


def parse_args(args) -> Tuple[str, Path, bool, bool]:
    parser = ArgumentParser(description="Download, convert and save documentation page")
    parser.add_argument("url", nargs=1, help="Documentation URL to download")
    parser.add_argument(
        "--output-dir", help="Directory to save the page", default=os.getcwd()
    )
    parser.add_argument(
        "-v", "--verbose", help="Be more verbose", action="store_true", default=False
    )
    parser.add_argument(
        "-f",
        "--force",
        help="Overwrite existing file when it exists",
        action="store_true",
        default=False,
    )

    parsed_args = parser.parse_args(args)

    return (
        parsed_args.url[0],
        Path(parsed_args.output_dir),
        parsed_args.force,
        parsed_args.verbose,
    )


def convert_title_to_filename(title: str) -> str:
    return slugify(title)


def replace_old_toc_with_new_toc(content: str) -> str:
    parsed_lines = []
    stripping_toc = False
    stripping_done = False
    toc_placeholder = "___toc_placeholder___"

    for line in content.splitlines():
        stripped_line = line.strip()

        if stripped_line == "**TABLE OF CONTENTS**" and not stripping_done:
            stripping_toc = True
            parsed_lines.append(toc_placeholder)
            continue

        if stripping_toc:
            if not stripped_line:
                continue
            if (
                stripped_line.startswith("* [")
                or stripped_line.startswith("+ [")
                or stripped_line.startswith("- [")
            ):
                continue
            else:
                stripping_toc = False
                stripping_done = True

        if not stripping_toc:
            parsed_lines.append(line)

    if stripping_toc and not stripping_done:
        raise Exception("Unfinished Table of Contents found!")

    content = "\n".join(parsed_lines)

    SPHINX_TOC = """
    ## Table of Contents
    ```{contents}
    :depth: 3
    :backlinks: none
    ```
    """

    content = content.replace(toc_placeholder, dedent(SPHINX_TOC).lstrip())

    return content


def main(args: List[str]) -> int:
    url, output_dir, force, verbose = parse_args(args)

    configure_logging(verbose)

    if not os.path.isdir(output_dir):
        logger.error(f"Output directory {output_dir} does not exist!")
        return os.EX_USAGE

    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    article_heading = soup.find(class_="hc-heading").text
    article_body = soup.find(id="article-body")

    def code_language_cb(el):
        if not el.has_attr("class"):
            return None
        for class_name in el["class"]:
            if class_name.startswith("language-"):
                return class_name.replace("language-", "")
        return None

    article_body_markdown = md(
        str(article_body), code_language_callback=code_language_cb
    )

    # Remove trailing whitespace from lines
    TRAILING_WHITESPACE_PATTERIN = re.compile(r"[ ]+\n")
    article_body_markdown = TRAILING_WHITESPACE_PATTERIN.sub(
        "\n", article_body_markdown
    )

    # Remove unnecessary empty lines
    DUPLICATE_EMPTY_LINE_PATTERN = re.compile(r"[\n]{3,}")
    article_body_markdown = DUPLICATE_EMPTY_LINE_PATTERN.sub(
        "\n\n", article_body_markdown
    )

    article_body_markdown = replace_old_toc_with_new_toc(article_body_markdown)

    def download_image_cb(match: re.Match) -> str:
        image_url = match.group(1)
        r = urlparse(image_url)
        filename = basename(r.path)

        res_dir = output_dir.joinpath("_res")
        filepath = res_dir.joinpath(filename)
        markdown_image_url = f"![](_res/{filename})"
        if os.path.isfile(filepath):
            return markdown_image_url

        if not os.path.isdir(res_dir):
            os.mkdir(res_dir)

        res_response = requests.get(image_url)
        # @NOTE(timon): Perhaps check mime type?
        with open(filepath, mode="wb") as f:
            f.write(res_response.content)

        return markdown_image_url

    IMAGE_PATTERN = re.compile(r"!\[\]\((.+)\)")
    article_body_markdown = IMAGE_PATTERN.sub(download_image_cb, article_body_markdown)

    filename = convert_title_to_filename(article_heading)
    filepath = output_dir.joinpath(filename + ".md")
    document_source_comment = f"<!-- source: {url} -->"
    document_contents = (
        f"{document_source_comment}"
        "\n"
        f"# {article_heading}"
        "\n"
        f"{article_body_markdown}"
    )

    if os.path.isfile(filepath):
        if force:
            logger.warning(f"Overwriting file {filepath}")
        else:
            logger.error(f"File {filepath} already exists! Exiting")
            return os.EX_USAGE

    with open(filepath, mode="w", encoding="utf-8") as f:
        logger.warning(f"Writing converted doc to {filepath}")
        f.write(document_contents)

    return os.EX_OK


if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
