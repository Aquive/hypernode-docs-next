#!/usr/bin/env python3

import sys
from argparse import ArgumentParser
from logging import DEBUG, INFO, StreamHandler, getLogger
from typing import List, Tuple
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup

logger = getLogger(__name__)


def configure_logging(debug: bool) -> None:
    level = DEBUG if debug else INFO
    logger.setLevel(level)
    ch = StreamHandler()
    logger.addHandler(ch)


def parse_args(args) -> Tuple[bool]:
    parser = ArgumentParser(description="Download, convert and save documentation page")
    parser.add_argument(
        "-v", "--verbose", help="Be more verbose", action="store_true", default=False
    )

    parsed_args = parser.parse_args(args)

    return (parsed_args.verbose,)


scraped_urls = []


def scrape(url: str) -> List[str]:
    try:
        logger.info(f"Fetching {url}")
        response = requests.get(url)
    except Exception:
        logger.warning(f"Failed to fetch {url}")
        return []
    result = [url]
    url_parts = urlparse(url)
    soup = BeautifulSoup(response.content, "html.parser")
    links = soup.find_all(name="a")
    for link in links:
        if link.has_attr("href"):
            link_url = link["href"]
            if link_url.startswith("/"):
                link_url = f"{url_parts.scheme}://{url_parts.hostname}{link_url}"
            elif link_url.startswith("http://") or link_url.startswith("https://"):
                link_url = link_url
            else:
                continue
            link_url = link_url.split("#")[0].rstrip("/")
            if "?" not in link_url:
                link_url = link_url + "/"
            link_url_parts = urlparse(link_url)
            if (
                link_url_parts.hostname != url_parts.hostname
                or not link_url_parts.path.startswith("/en/")
            ):
                continue
            if link_url not in scraped_urls:
                scraped_urls.append(link_url)
                result.extend(scrape(link_url))

    return result


def main(args: List[str]) -> int:
    verbose = parse_args(args)
    configure_logging(verbose)
    result = sorted(scrape("https://support.hypernode.com/en/"))
    with open("documentation_urls.txt", "w", encoding="utf-8") as f:
        for link in result:
            f.write(f"{link}\n")


if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
